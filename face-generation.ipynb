{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = (\n",
    "    \"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LR = 0.005\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 128\n",
    "EPOCHS = 50\n",
    "KLD_WEIGHT = 0.00025\n",
    "\n",
    "DATASET_PATH =r\" C:\\Users\\ACER\\Downloads\\img_align_celeba\\img_align_celeba\"\n",
    "DATASET_ATTRS_PATH = \"dataset/list_attr_celeba.csv\"\n",
    "\n",
    "NUM_FRAMES = 50\n",
    "FPS = 5\n",
    "\n",
    "LABELS = [\"Eyeglasses\", \"Smiling\", \"Attractive\", \"Male\", \"Blond_Hair\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.all_images = list(glob.iglob(root_dir + \"/*.jpg\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "    def __shape__(self):\n",
    "        return self.shape\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.all_images[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "matplotlib.use(\"Agg\")\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(VAELossParams, kld_weight):\n",
    "    recons,input,mu,log_var=VAELossParams\n",
    "    recons_loss=F.mse_loss(recons,input)\n",
    "     \n",
    "    kld_loss=torch.mean(-0.5 * torch.sum(1+log_var-mu**2-log_var.exp(),dim=1),dim=0)\n",
    "    loss= recons_loss + (kld_weight*kld_loss)\n",
    "    return{\"reconstruction loss\":recons_loss,\"kld loss\":kld_loss,\"loss\":loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Tuple\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self,in_channels:int,out_channels:int) ->None:\n",
    "        super(ConvBlock,self).__init__()\n",
    "        self.block=nn.Sequential(nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1,\n",
    "        ),\n",
    "        nn.BatchNorm2d(out_channels),nn.LeakyReLU(),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.block(x)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvTBlock(nn.Module):\n",
    "    def __init__(self,in_channels:int,out_channels:int) ->None:\n",
    "        super(ConvTBlock,self).__init__()\n",
    "        self.block=nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=1,\n",
    "\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.LeakyReLU()\n",
    "\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.block(x)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class celebVAE(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, latent_dim: int, hidden_dims: List[int] = None) -> None:\n",
    "        super(celebVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            *[\n",
    "                ConvBlock(in_f, out_f)\n",
    "                for in_f, out_f in zip([in_channels] + hidden_dims[:-1], hidden_dims)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1] * 4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1] * 4, latent_dim)\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            *[\n",
    "                ConvTBlock(in_f, out_f)\n",
    "                for in_f, out_f in zip(hidden_dims[:-1], hidden_dims[1:])\n",
    "            ]\n",
    "        )\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dims[-1],\n",
    "                hidden_dims[-1],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], out_channels=3, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def encode(self, input: torch.Tensor) -> List[Tuple[torch.Tensor]]:\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "        return mu, log_var\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 32, 8, 8)  \n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "\n",
    "    def reparametrize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return (eps * std + mu)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparametrize(mu, log_var)\n",
    "        return [self.decode(z), input, mu, log_var]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=\"output\"\n",
    "os.makedirs(\"output\",exist_ok=True)\n",
    "training_progress_dir=os.path.join(output_dir,\"training_progress\")\n",
    "os.makedirs(training_progress_dir,exist_ok=True)\n",
    "model_weights_dir=os.path.join(output_dir,\"model weights\")\n",
    "os.makedirs(model_weights_dir,exist_ok=True)\n",
    "MODEL_BEST_WEIGHTS_PATH=os.path.join(model_weights_dir,\"best_vae_celeba.pt\")\n",
    "MODEL_WEIGHTS_PATH=os.path.join(model_weights_dir,\"vae_celeba.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms=transforms.Compose(\n",
    "\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(148),\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "validation_transforms=transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(148),\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "celeba_dataset = CelebADataset(root_dir=r\"C:\\Users\\ACER\\Downloads\\img_align_celeba\", transform=train_transforms)\n",
    "\n",
    "val_size=int(len(celeba_dataset)*0.1)\n",
    "train_size=len(celeba_dataset)-val_size\n",
    "\n",
    "\n",
    "train_dataset,val_dataset=random_split(celeba_dataset,[train_size,val_size])\n",
    "\n",
    "train_dataloader=DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_dataloader=DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter=iter(train_dataloader)\n",
    "batch=next(data_iter)\n",
    "#for images in batch:\n",
    "   # print(images.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11892\n",
      "torch.Size([3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "celeba_dataset = CelebADataset(root_dir=r\"C:\\Users\\ACER\\Downloads\\img_align_celeba\", transform=train_transforms)\n",
    "print(len(celeba_dataset))\n",
    "print(celeba_dataset[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_10376\\618999298.py:12: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as vutils\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    images = batch \n",
    "    for images in batch:\n",
    "        \n",
    "        plt.imshow(vutils.make_grid(images, normalize=True).permute(1, 2, 0))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Sample Images\")\n",
    "        plt.show()\n",
    "        plt.savefig('plot.png')\n",
    "        break\n",
    "\n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=celebVAE(in_channels=CHANNELS,out_channels=None,latent_dim=EMBEDDING_DIM,hidden_dims=None,)\n",
    "model=model.to(DEVICE)\n",
    "optimizer=optim.SGD(model.parameters(),lr=LR)\n",
    "scheduler=optim.lr_scheduler.ExponentialLR(optimizer,gamma=0.95)\n",
    "\n",
    "del model\n",
    "\n",
    "best_val_loss=float(\"inf\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss=0.0\n",
    "    for i,x in enumerate(train_dataloader):\n",
    "\n",
    "        print(x.shape)\n",
    "        x=x.to(DEVICE)\n",
    "        del x\n",
    "        optimizer.zero_grad()\n",
    "        predicted_values=model(x)\n",
    "        total_loss=loss_function(predicted_values,KLD_WEIGHT)\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss+=total_loss.item()\n",
    "    print(\"epoch \",epoch)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss=running_loss/len(train_dataloader)\n",
    "for i,x in enumerate(val_dataloader):\n",
    "        x=x.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        predicted_values=model(x)\n",
    "        val_loss=(loss_function(predicted_values,KLD_WEIGHT)).mean()\n",
    "        \n",
    "        \n",
    "torch.save({\"vae-celeba\": model.state_dict()},\n",
    "           MODEL_BEST_WEIGHTS_PATH,) \n",
    "\n",
    "torch.save({\"vae-celeba\":model.state_dict()},\n",
    "           MODEL_WEIGHTS_PATH,)\n",
    "\n",
    "print(\n",
    "        f\"Epoch {epoch+1}/{EPOCHS}, Batch {i+1}/{len(train_dataloader)}, \"\n",
    "        f\"Total Loss: {total_loss['loss'].detach().item():.4f}, \"\n",
    "        f\"Reconstruction Loss: {total_loss['Reconstruction_Loss']:.4f}, \"\n",
    "        f\"KL Divergence Loss: {total_loss['KLD']:.4f}\",\n",
    "        f\"Val Loss: {val_loss:.4f}\",\n",
    "    )\n",
    "scheduler.step()\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
